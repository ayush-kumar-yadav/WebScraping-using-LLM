{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca878ec-4e0d-416a-9071-8b7ebf08aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #Access environment variables like API keys (os.getenv()), file paths.\n",
    "import requests#Make HTTP requests to fetch webpage content and send POST requests to Ollama API.\n",
    "import json#Parse or format JSON data (for input/output with Ollama).\n",
    "from typing import List#From typing, used for type hinting lists in functions or classes.\n",
    "from dotenv import load_dotenv#Load variables from .env file into environment (good for secrets like API keys).\n",
    "from bs4 import BeautifulSoup#Parses HTML for easy text and tag extraction.\n",
    "from IPython.display import Markdown, display, update_display#Used in Jupyter Notebooks to show nicely formatted markdown outputs\n",
    "from json import JSONDecodeError\n",
    "from urllib.parse import urlparse, urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c52e53f-fa45-4bfe-87d4-2fea5600ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "#Defines which local model you’ll use via Ollama (llama2, mistral, gemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f4fa33-81b8-4b21-9cb5-b361e3649728",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={\n",
    "    \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}#Some websites block scrapers. This makes your request look like it's from a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6bda684-2a5f-4f87-b21b-9dc5d0edb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.title = \"No title found\"\n",
    "        self.text = \"\"\n",
    "        self.links = []\n",
    "        \n",
    "        try:\n",
    "            # Add timeout to prevent hanging\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            self.body = response.content\n",
    "            soup = BeautifulSoup(self.body, 'html.parser')\n",
    "            \n",
    "            self.title = soup.title.string if soup.title else \"No title found\"\n",
    "            \n",
    "            if soup.body:\n",
    "                for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                    tag.decompose()\n",
    "                self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Extract and normalize links\n",
    "            base_url = self.get_base_url(url)\n",
    "            raw_links = [link.get('href') for link in soup.find_all('a')]\n",
    "            self.links = [self.normalize_url(link, base_url) for link in raw_links if link]\n",
    "            \n",
    "        except (requests.RequestException, Exception) as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            # Initialize with empty/default values on error\n",
    "    \n",
    "    def get_base_url(self, url):\n",
    "        \"\"\"Extract base URL for resolving relative URLs\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    \n",
    "    def normalize_url(self, link, base_url):\n",
    "        \"\"\"Convert relative URLs to absolute\"\"\"\n",
    "        if not link:\n",
    "            return None\n",
    "        \n",
    "        # Skip mailto: links, javascript:, etc.\n",
    "        if link.startswith(('mailto:', 'javascript:', 'tel:')):\n",
    "            return None\n",
    "            \n",
    "        # Handle fragment-only links\n",
    "        if link.startswith('#'):\n",
    "            return None\n",
    "        \n",
    "        # Convert relative to absolute\n",
    "        if not link.startswith(('http://', 'https://')):\n",
    "            return urljoin(base_url, link)\n",
    "        \n",
    "        return link\n",
    "    \n",
    "    def get_contents(self):\n",
    "        return f\"webpage title:\\n{self.title}\\nwebpage contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "025a121e-15cf-4e82-bb0b-3841fddc41e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links...You should respond in JSON as in this example:\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
      "        {\"type\": \"careers page\": \"url\":\"https://another.full.url/careers\"}\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link_system_prompt = \"\"\"You are provided with a list of links...\"\"\"#A system prompt that instructs the LLM how to identify important links (e.g., About, Careers).\n",
    "#✅ Use this when giving clear roles to the LLM for specific tasks (like classification or summarization).\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\": \"url\":\"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee03bef9-f77e-44ad-b193-7674cb2b88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt\n",
    "    #Takes the scraped links and converts them into a natural prompt for the LLM to decide relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e458fc-5dcb-41a3-be2b-4b34d9367842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_links(url):\n",
    "    site_obj = website(url)  # Updated class name\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(site_obj)}\n",
    "        ]\n",
    "    }\n",
    "    resp = requests.post(OLLAMA_API, json=payload, stream=True)\n",
    "    # 1) Accumulate all the chunks into one string\n",
    "    raw = \"\"\n",
    "    for line in resp.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        chunk = json.loads(line.decode(\"utf-8\"))\n",
    "        raw += chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "    # 2) Strip code fences/backticks\n",
    "    cleaned = raw.replace(\"```\", \"\").strip()\n",
    "    # 3) Extract the JSON object using regex\n",
    "    match = re.search(r\"\\{.*\\}\", cleaned, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        raise RuntimeError(f\"Failed to extract JSON. Here's a snippet:\\n{cleaned[:200]}…\")\n",
    "    # 4) Extracted JSON string\n",
    "    json_str = match.group(0)\n",
    "    # 5) Parse and return the JSON data\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Failed to parse JSON:\\n{json_str[:200]}...\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08ec2ec1-2b74-42ab-bb09-1a448076e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(url):\n",
    "    result = \"landing page:\\n\"\n",
    "    try:\n",
    "        landing_page = Website(url)\n",
    "        result += landing_page.text\n",
    "        \n",
    "        # Try to get links, but handle errors\n",
    "        try:\n",
    "            links = get_links(url)\n",
    "            if links and \"links\" in links:\n",
    "                for link in links[\"links\"]:\n",
    "                    try:\n",
    "                        link_url = link.get(\"url\")\n",
    "                        if link_url:\n",
    "                            result += f\"\\n\\n{link['type']}\\n\"\n",
    "                            result += Website(link_url).get_contents()\n",
    "                    except Exception as e:\n",
    "                        result += f\"\\n\\nError fetching {link.get('type', 'unknown')} page: {str(e)}\\n\"\n",
    "        except Exception as e:\n",
    "            result += f\"\\n\\nError getting links: {str(e)}\\n\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        result += f\"Error fetching main page: {str(e)}\\n\"\n",
    "        \n",
    "    return result\n",
    "#Returns the full combined content of:\n",
    "#The landing page, and\n",
    "#The important subpages selected by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "067fe5a1-3a89-4140-bf6c-a76d105817b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant that analyzes the contents...\"\"\"\n",
    "def get_brochure_user_prompt(company_name, url):\n",
    "    prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "    prompt += \"Here are the contents...\"\n",
    "    prompt += get_all_details(url)\n",
    "    return prompt[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d3a4044-9d57-4528-9297-36e9ac16c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt(company_name, url):\n",
    "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "752a27f2-454d-4770-b4ce-91228de2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "        ]\n",
    "    }\n",
    "    resp = requests.post(OLLAMA_API, json=payload)\n",
    "    try:\n",
    "        data = resp.json()\n",
    "        if \"response\" in data:\n",
    "            brochure_md = data[\"response\"]\n",
    "        elif \"message\" in data and \"content\" in data[\"message\"]:\n",
    "            brochure_md = data[\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise KeyError(f\"Couldn't find a response in:\\n{data}\")\n",
    "    except JSONDecodeError:\n",
    "        # Handle streaming response if needed\n",
    "        raw = \"\"\n",
    "        for line in resp.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                chunk = json.loads(line.decode(\"utf-8\"))\n",
    "                raw += chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "            except:\n",
    "                continue\n",
    "        brochure_md = raw\n",
    "    \n",
    "    display(Markdown(brochure_md))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8db8852-878b-4c65-9f77-096e8b388587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unfortunately, it seems like there's an issue with the content. However, I can still provide a generic brochure for Hugging Face based on my general knowledge.\n",
       "\n",
       "**Hugging Face**\n",
       "================\n",
       "\n",
       "### About Us\n",
       "\n",
       "Hugging Face is a leading open-source software company that provides tools and technologies for natural language processing (NLP) and machine learning.\n",
       "\n",
       "### Our Mission\n",
       "\n",
       "Empowering developers to build intelligent applications through cutting-edge NLP and ML libraries.\n",
       "\n",
       "### What We Do\n",
       "\n",
       "* Develop and maintain popular open-source libraries such as Transformers, Hugging Face Datasets, and SparkNLP.\n",
       "* Provide pre-trained models for a wide range of NLP tasks, including language modeling, sentiment analysis, and text classification.\n",
       "* Offer a suite of tools for building and deploying ML models, including the popular Transformers library.\n",
       "\n",
       "### Our Products\n",
       "\n",
       "#### **Transformers**\n",
       "\n",
       "A popular open-source library for transformer-based architectures, offering state-of-the-art performance in NLP tasks.\n",
       "\n",
       "#### **Hugging Face Datasets**\n",
       "\n",
       "A comprehensive collection of pre-processed datasets for various NLP tasks, making it easy to get started with your projects.\n",
       "\n",
       "#### **SparkNLP**\n",
       "\n",
       "A Java library that provides a simple and efficient way to build ML models for text data.\n",
       "\n",
       "### Why Choose Us\n",
       "\n",
       "* **Community-driven**: Our libraries are built and maintained by a passionate community of developers.\n",
       "* **High-performance**: Our models are optimized for speed and accuracy, making them perfect for production environments.\n",
       "* **Easy to use**: Our libraries provide simple and intuitive APIs, making it easy to get started with NLP and ML.\n",
       "\n",
       "### Join the Community\n",
       "\n",
       "* Visit our [GitHub page](https://github.com/huggingface) to explore our open-source projects.\n",
       "* Follow us on [Twitter](https://twitter.com/huggingface) for updates and news.\n",
       "* Join our [Discord community](https://discord.huggingface.co/) to connect with other developers and experts.\n",
       "\n",
       "### Get Started\n",
       "\n",
       "Visit our [website](http://huggingface.co) to learn more about our products and libraries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_brochure(\"HuggingFace\", \"https://huggingface.co\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85abd9-bc6d-4aa6-8865-31e46d1a5d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc8132-e387-41b3-988d-fca51530573c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
